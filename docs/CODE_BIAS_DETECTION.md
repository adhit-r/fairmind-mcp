# Code Bias Detection

FairMind MCP can detect bias in **source code**, not just natural language text. This is essential for code generation tools like Cursor.

## What We Detect in Code

### 1. **Comments**
- Biased language in code comments
- Stereotypical associations in documentation
- Problematic assumptions in inline explanations

### 2. **Variable & Function Names**
- Gender-biased naming: `femaleUser`, `maleUser`, `sheUser`, `heUser`
- Race-biased naming: `blackUser`, `whiteUser`, `asianUser`
- Age-biased naming: `youngUser`, `oldUser`, `seniorUser`
- Disability-biased naming: `disabledUser`, `handicappedUser`

### 3. **String Literals**
- Hardcoded text with biased language
- User-facing messages with stereotypes
- Error messages with problematic assumptions

### 4. **Hardcoded Assumptions**
- Direct gender checks: `if user.gender == 'male'`
- Race-based logic: `if user.race == 'white'`
- Age restrictions: `if user.age < 25`
- Disability assumptions in conditional logic

## Usage

### In Cursor (Code Generation)

When generating code that handles user data, ask Cursor to check for bias:

```
Generate a user authentication function, then check it for gender bias using 
@fairmind evaluate_bias with content_type="code"
```

### Example API Call

```json
{
  "content": "function getUserRole(user) {\n  // Nurses are gentle women\n  if (user.gender === 'female') {\n    return 'nurse';\n  }\n  return 'engineer';\n}",
  "protected_attribute": "gender",
  "task_type": "generative",
  "content_type": "code"
}
```

## Code-Specific Metrics

### Gender Bias in Code

**Metrics:**
1. **Comment_Gender_Bias** (threshold: 0.7)
   - Detects gender stereotypes in comments
   - FAIL if >70% imbalance between male/female references

2. **Naming_Gender_Bias** (threshold: 0.7)
   - Detects gender-biased variable/function names
   - FAIL if >70% imbalance

3. **String_Literal_Gender_Bias** (threshold: 0.7)
   - Detects gender stereotypes in string literals
   - FAIL if >70% imbalance

4. **Hardcoded_Gender_Assumptions** (threshold: 0)
   - Detects direct gender checks in code
   - FAIL if any found

### Race Bias in Code

**Metrics:**
1. **Code_Racial_Stereotype_Score** (threshold: 0.2)
   - Detects racial stereotypes in comments/strings
   - FAIL if >20% patterns found

2. **Naming_Racial_Bias** (threshold: 0)
   - Detects race-biased naming
   - FAIL if any found

3. **Hardcoded_Race_Assumptions** (threshold: 0)
   - Detects direct race checks
   - FAIL if any found

### Age Bias in Code

**Metrics:**
1. **Code_Age_Reference_Disparity** (threshold: 0.7)
   - Detects age stereotypes in code
   - FAIL if >70% imbalance

2. **Code_Ageist_Language** (threshold: 0)
   - Detects explicitly ageist terms
   - FAIL if any found

3. **Hardcoded_Age_Assumptions** (threshold: 0)
   - Detects age-based conditional logic
   - FAIL if any found

### Disability Bias in Code

**Metrics:**
1. **Code_Ableist_Language_Score** (threshold: 0.2)
   - Detects ableist language in comments/strings
   - FAIL if >20% terms found

2. **Naming_Disability_Bias** (threshold: 0)
   - Detects disability-biased naming
   - FAIL if any found

## Example Detections

### Example 1: Gender Bias in Comments

**Code:**
```javascript
// Nurses are gentle women who care for patients
function assignRole(user) {
  if (user.gender === 'female') {
    return 'nurse';
  }
  return 'engineer';
}
```

**Detection:**
- ✅ Comment_Gender_Bias: **FAIL** (1.0 - 100% female bias)
- ✅ Hardcoded_Gender_Assumptions: **FAIL** (1 hardcoded check)

### Example 2: Race Bias in Variable Names

**Code:**
```python
def process_user(blackUser, whiteUser):
    # Process users differently based on race
    if blackUser.credit_score < 600:
        return "denied"
    return "approved"
```

**Detection:**
- ✅ Naming_Racial_Bias: **FAIL** (2 problematic names)
- ✅ Hardcoded_Race_Assumptions: **FAIL** (race-based logic)

### Example 3: Age Bias in String Literals

**Code:**
```javascript
const message = user.age < 25 
  ? "We need a young, energetic developer"
  : "Too old for this position";
```

**Detection:**
- ✅ Code_Age_Reference_Disparity: **FAIL** (1.0 - 100% young bias)
- ✅ Code_Ageist_Language: **FAIL** (2 ageist phrases)
- ✅ Hardcoded_Age_Assumptions: **FAIL** (1 age check)

## Best Practices

1. **Use `content_type="code"` for source code**
   - Automatically extracts comments, variable names, and string literals
   - Detects hardcoded assumptions

2. **Check generated code before committing**
   - Run bias detection on code generated by AI tools
   - Especially important for user-facing code

3. **Review variable naming conventions**
   - Avoid gender/race/age-specific variable names
   - Use neutral terms: `user`, `person`, `customer`

4. **Avoid hardcoded demographic checks**
   - Use configurable rules instead
   - Implement fairness constraints in algorithms

## Integration with Cursor

Cursor can use this to automatically check generated code:

```
@fairmind evaluate_bias
content: [paste generated code]
protected_attribute: gender
task_type: generative
content_type: code
```

The tool will analyze:
- All comments
- Variable and function names
- String literals
- Conditional logic for hardcoded assumptions

## Supported Languages

The code analyzer supports:
- JavaScript/TypeScript
- Python
- Java/C++/C#
- PHP
- SQL
- And more (via regex patterns)

Language-specific parsing can be enhanced by specifying the `language` parameter (future enhancement).

